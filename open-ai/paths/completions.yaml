base:

  post:

    operationId: engines.completions.create
    summary: Create completion
    description: |
      Creates a new completion for the provided prompt and parameters.

      [See More](https://beta.openai.com/docs/api-reference/completions/create)
    tags:
      - Completions
    security:
      - bearer: []
      - basic: []
    parameters:
      - $ref: ../index.yaml#/components/parameters/organization
      - $ref: ../index.yaml#/components/parameters/engineId
    requestBody:
      required: true
      content:
        application/json:
          schema:
            type: object
            properties:
              prompt:
                oneOf:
                  - type: string
                  - type: array
                    items:
                      type: string
                default: <|endoftext|>
                description: |
                  The prompt(s) to generate completions for, encoded as a string, a list of strings, or a list of token lists.

                  Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.
              max_tokens:
                type: integer
                minimum: 1
                maximum: 2048
                default: 16
                description: |
                  The maximum number of tokens to generate.
                  Requests can use up to 2048 tokens shared between prompt and completion.
                  (One token is roughly 4 characters for normal English text)
              temperature:
                type: number
                default: 1
                description: |
                  What [sampling temperature](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277) to use.
                  Higher values means the model will take more risks.
                  Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.

                  We generally recommend altering this or `top_p` but not both.
              top_p:
                type: number
                default: 1
                description: |
                  An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.
                  So 0.1 means only the tokens comprising the top 10% probability mass are considered.

                  We generally recommend altering this or `temperature` but not both.
              n:
                type: integer
                minimum: 1
                default: 1
                description: |
                  How many completions to generate for each prompt.

                  Note: Because this parameter generates many completions, it can quickly consume your token quota.
                  Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
              stream:
                type: boolean
                default: false
                description: |
                  Whether to stream back partial progress.
                  If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message.
              logprobs:
                type: integer
                nullable: true
                default: null
                description: |
                  Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens.
                  For example, if `logprobs` is 10, the API will return a list of the 10 most likely tokens.
                  The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.
              echo:
                type: boolean
                default: false
                description: |
                  Echo back the prompt in addition to the completion
              stop:
                oneOf:
                  - type: string
                  - type: array
                    items:
                      type: string
                type: string
                nullable: true
                default: null
                description: |
                  Up to 4 sequences where the API will stop generating further tokens.
                  The returned text will not contain the stop sequence.
                example: \n
              presence_penalty:
                type: number
                minimum: 0
                maximum: 1
                default: 0
                description: |
                  Number between 0 and 1 that penalizes new tokens based on whether they appear in the text so far.
                  Increases the model's likelihood to talk about new topics.

                  [See more information about frequency and presence penalties..](https://beta.openai.com/docs/api-reference/parameter-details)
              frequency_penalty:
                type: number
                minimum: 0
                maximum: 1
                default: 0
                description: |
                  Number between 0 and 1 that penalizes new tokens based on their existing frequency in the text so far.
                  Decreases the model's likelihood to repeat the same line verbatim.

                  [See more information about frequency and presence penalties..](https://beta.openai.com/docs/api-reference/parameter-details)
              best_of:
                type: integer
                minimum: 1
                default: 1
                description: |
                  Generates `best_of` completions server-side and returns the "best" (the one with the lowest log probability per token).
                  Results cannot be streamed.

                  When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return â€“ `best_of` must be greater than `n`.

                  __Note:__ Because this parameter generates many completions, it can quickly consume your token quota.
                  Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
              logit_bias:
                $ref: ../index.yaml#/components/schemas/logitBias
            additionalProperties: false
    responses:
      200:
        description: Ok
        content:
          application/json:
            schema:
              type: object
              properties:
                id:
                  type: string
                  example: cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7
                object:
                  type: string
                  example: text_completion
                created:
                  type: integer
                  description: Unix Seconds.
                  example: 1589478378
                model:
                  type: string
                  example: davinci:2020-05-03
                choices:
                  type: array
                  items:
                    type: object
                    properties:
                      text:
                        type: string
                      index:
                        type: integer
                      logprobs:
                        type: integer
                        nullable: true
                      finish_reason:
                        type: string
                    additionalProperties: false
                    required:
                      - text
                      - index
                      - logprobs
                      - finish_reason
                  example:
                    - text: " there was a girl who"
                      index: 0
                      logprobs: null
                      finish_reason: length
              additionalProperties: false
              required:
                - id
                - object
                - created
                - model
                - choices
